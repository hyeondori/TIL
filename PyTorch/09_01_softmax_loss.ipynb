{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5217b8b0-0efb-4921-888a-327d77b6b6dd",
   "metadata": {},
   "source": [
    "*[본 포스팅은 김성훈 교수님의 PyTorch Zero To All Lecture을 보며 작성한 글입니다]*\n",
    "\n",
    "## 들어가기 전에...\n",
    "\n",
    "앞선 포스팅에서는 Output size가 1인 경우에 대해서만 다뤘다. 그러나 우리 주변에는 Output size가 그 이상인 경우가 훨씬 많다. 아주 basic한 dataset으로 불려지는 MNIST 데이터셋 또한 0부터 9까지의 10가지 label을 갖고 있어 10개의 Output을 필요로 한다. 이 경우 우리는 Activation Function으로 Softmax함수를 활용하게 된다.\n",
    "\n",
    "Softmax 함수는 다음과 같이 구성된다.\n",
    "\n",
    "### Softmax 함수\n",
    "\n",
    "$$\n",
    "p_j=\\frac{e^{z_j}}{\\displaystyle\\sum^K_{k=1}e^{z_j}}\\\\j = 1, 2, ..., K\n",
    "$$\n",
    "\n",
    "Softmax 함수의 작동 원리를 설명하는 것은 다소 길어질 수 있어 따로 정리하도록 하겠다.\n",
    "\n",
    "중요한 점은 Softmax Function의 결과는 원하는 Output 결과 갯수만큼의 0과 1 사이의 값으로 떨어지게 되고, 이 값들의 합은 1이 된다.\n",
    "\n",
    "Multi-class classification 문제를 해결하기 위해 softmax 함수와 함께 사용하는 Loss Function은 Cross Entropy Loss이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557b441-ea3f-4bc3-9509-1bfd6daec10c",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc528e7-9972-4fe3-ba87-f087b034f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4136072-deb0-48f5-b7f0-da631e016139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b65f376-11d3-49f2-bb49-f34477294fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST Model on cuda\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b59562-947f-42d1-ab34-2360483bd284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "28.6%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c8b9175-008d-4e76-87c8-452201270167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5354ef2a-9ad0-44ee-b583-11d6ba03c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c7f547-3995-400a-b87d-a3e97947a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd7f978b-1087-4b01-b63a-aa517b2f6877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.296072\n",
      "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.295131\n",
      "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.301317\n",
      "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.314219\n",
      "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.298035\n",
      "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.304544\n",
      "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.287348\n",
      "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.293203\n",
      "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.300339\n",
      "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.306266\n",
      "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.291584\n",
      "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.287301\n",
      "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.292174\n",
      "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.284783\n",
      "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.301886\n",
      "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.296906\n",
      "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.305664\n",
      "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.303731\n",
      "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.291594\n",
      "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.292278\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.304258\n",
      "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.288866\n",
      "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.295130\n",
      "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.298099\n",
      "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.297918\n",
      "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.289690\n",
      "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.295086\n",
      "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.293071\n",
      "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.297121\n",
      "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.286344\n",
      "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.281113\n",
      "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.281066\n",
      "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.283234\n",
      "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.295396\n",
      "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.288795\n",
      "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.295697\n",
      "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.297215\n",
      "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.294091\n",
      "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.278010\n",
      "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.283611\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.293106\n",
      "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.293073\n",
      "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.287701\n",
      "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.279325\n",
      "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.275893\n",
      "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.283577\n",
      "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.282662\n",
      "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.283823\n",
      "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.273261\n",
      "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.283561\n",
      "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.273527\n",
      "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.280375\n",
      "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.270526\n",
      "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.261835\n",
      "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.267290\n",
      "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.263947\n",
      "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.262489\n",
      "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.262458\n",
      "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.259764\n",
      "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.257237\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.252460\n",
      "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.259271\n",
      "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.235869\n",
      "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.268433\n",
      "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.228321\n",
      "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.219825\n",
      "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.234022\n",
      "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.245990\n",
      "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.227678\n",
      "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.220353\n",
      "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.202276\n",
      "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.207091\n",
      "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.231782\n",
      "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.172533\n",
      "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.153572\n",
      "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.198736\n",
      "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.174287\n",
      "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.139524\n",
      "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.141041\n",
      "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 2.117168\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.112450\n",
      "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 2.098913\n",
      "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 1.982769\n",
      "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 2.071138\n",
      "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 1.938501\n",
      "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 1.993228\n",
      "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.913634\n",
      "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 1.880913\n",
      "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.918134\n",
      "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.919612\n",
      "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.779679\n",
      "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.685662\n",
      "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 1.771263\n",
      "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.683455\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0258, Accuracy: 4496/10000 (45%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.735987\n",
      "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 1.546965\n",
      "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.810208\n",
      "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.520556\n",
      "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 1.481023\n",
      "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 1.390683\n",
      "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 1.347581\n",
      "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 1.325665\n",
      "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 1.179341\n",
      "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 1.305026\n",
      "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 1.074299\n",
      "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 1.018126\n",
      "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 1.336833\n",
      "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 1.054055\n",
      "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 1.064361\n",
      "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 1.081183\n",
      "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 1.047977\n",
      "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 1.050890\n",
      "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.952464\n",
      "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.913854\n",
      "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.864179\n",
      "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 1.025241\n",
      "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.746288\n",
      "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 1.052008\n",
      "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.889081\n",
      "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.891375\n",
      "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.819735\n",
      "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.923376\n",
      "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.942949\n",
      "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.806783\n",
      "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.808976\n",
      "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 1.138151\n",
      "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.743293\n",
      "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.749629\n",
      "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.738718\n",
      "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.790997\n",
      "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.642435\n",
      "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.705033\n",
      "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.749488\n",
      "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.762337\n",
      "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.650270\n",
      "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.511136\n",
      "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.798679\n",
      "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.775044\n",
      "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.663630\n",
      "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.611753\n",
      "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.895576\n",
      "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.719408\n",
      "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.765270\n",
      "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.682244\n",
      "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.605641\n",
      "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.470218\n",
      "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.643020\n",
      "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.847209\n",
      "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.627875\n",
      "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.816537\n",
      "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.885339\n",
      "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.474693\n",
      "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.735570\n",
      "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.545465\n",
      "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.472362\n",
      "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.566685\n",
      "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.574373\n",
      "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.732320\n",
      "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.437299\n",
      "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.649009\n",
      "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.485437\n",
      "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.747227\n",
      "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.438567\n",
      "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.490696\n",
      "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.456133\n",
      "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.475532\n",
      "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.528457\n",
      "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.546752\n",
      "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.581357\n",
      "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.543617\n",
      "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.607232\n",
      "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.333322\n",
      "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.599100\n",
      "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.428473\n",
      "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.369790\n",
      "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.495344\n",
      "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.588687\n",
      "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.572777\n",
      "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.432664\n",
      "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.536954\n",
      "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.560230\n",
      "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.406978\n",
      "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.571958\n",
      "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.620234\n",
      "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.539276\n",
      "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.580727\n",
      "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.447639\n",
      "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.488990\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0070, Accuracy: 8702/10000 (87%)\n",
      "Testing time: 0m 11s\n",
      "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.436990\n",
      "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.646177\n",
      "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.338354\n",
      "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.444983\n",
      "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.774906\n",
      "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.395108\n",
      "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.595982\n",
      "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.517784\n",
      "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.366404\n",
      "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.324308\n",
      "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.264630\n",
      "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.443152\n",
      "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.350922\n",
      "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.299974\n",
      "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.417519\n",
      "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.646004\n",
      "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.407587\n",
      "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.376966\n",
      "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.486509\n",
      "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.364843\n",
      "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.320583\n",
      "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.538596\n",
      "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.542291\n",
      "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.488254\n",
      "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.316842\n",
      "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.260216\n",
      "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.333344\n",
      "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.282028\n",
      "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.316107\n",
      "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.256297\n",
      "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.358967\n",
      "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.435984\n",
      "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.338033\n",
      "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.428781\n",
      "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.512357\n",
      "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.675719\n",
      "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.269458\n",
      "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.338913\n",
      "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.333076\n",
      "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.182370\n",
      "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.300801\n",
      "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.268688\n",
      "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.330364\n",
      "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.315152\n",
      "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.375296\n",
      "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.491638\n",
      "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.340481\n",
      "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.523859\n",
      "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.205655\n",
      "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.394768\n",
      "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.161253\n",
      "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.223771\n",
      "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.353970\n",
      "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.259306\n",
      "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.575195\n",
      "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.261299\n",
      "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.416057\n",
      "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.308564\n",
      "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.390484\n",
      "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.322140\n",
      "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.291994\n",
      "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.392502\n",
      "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.242208\n",
      "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.310120\n",
      "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.490597\n",
      "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.257696\n",
      "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.389701\n",
      "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.340446\n",
      "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.240350\n",
      "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.384595\n",
      "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.302630\n",
      "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.165341\n",
      "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.524848\n",
      "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.264504\n",
      "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.383661\n",
      "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.220496\n",
      "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.398892\n",
      "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.286033\n",
      "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.173927\n",
      "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.454738\n",
      "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.167564\n",
      "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.299716\n",
      "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.428423\n",
      "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.277550\n",
      "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.293292\n",
      "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.176592\n",
      "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.463638\n",
      "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.234036\n",
      "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.270519\n",
      "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.262370\n",
      "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.216109\n",
      "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.290315\n",
      "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.430780\n",
      "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.413089\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0047, Accuracy: 9143/10000 (91%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.244026\n",
      "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.319549\n",
      "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.186690\n",
      "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.300245\n",
      "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.255783\n",
      "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.276934\n",
      "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.309602\n",
      "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.085089\n",
      "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.191274\n",
      "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.237996\n",
      "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.340634\n",
      "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.232033\n",
      "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.412444\n",
      "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.176465\n",
      "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.372032\n",
      "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.272323\n",
      "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.218536\n",
      "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.202952\n",
      "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.235980\n",
      "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.118504\n",
      "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.295163\n",
      "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.152871\n",
      "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.335638\n",
      "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.392809\n",
      "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.189502\n",
      "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.218810\n",
      "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.250928\n",
      "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.331675\n",
      "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.258049\n",
      "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.175165\n",
      "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.253034\n",
      "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.327679\n",
      "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.249200\n",
      "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.167433\n",
      "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.203127\n",
      "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.209480\n",
      "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.432552\n",
      "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.100183\n",
      "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.196407\n",
      "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.306338\n",
      "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.237087\n",
      "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.248783\n",
      "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.098836\n",
      "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.215286\n",
      "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.378985\n",
      "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.413705\n",
      "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.275214\n",
      "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.307658\n",
      "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.192761\n",
      "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.165606\n",
      "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.270917\n",
      "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.298696\n",
      "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.152714\n",
      "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.139072\n",
      "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.246729\n",
      "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.126290\n",
      "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.180142\n",
      "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.327864\n",
      "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.191628\n",
      "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.403103\n",
      "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.408596\n",
      "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.200947\n",
      "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.268176\n",
      "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.206867\n",
      "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.108300\n",
      "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.460307\n",
      "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.232601\n",
      "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.089581\n",
      "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.191027\n",
      "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.260114\n",
      "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.368256\n",
      "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.228555\n",
      "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.342826\n",
      "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.260228\n",
      "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.137683\n",
      "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.465007\n",
      "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.193556\n",
      "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.297156\n",
      "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.089684\n",
      "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.259650\n",
      "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.537812\n",
      "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.236266\n",
      "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.228025\n",
      "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.226102\n",
      "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.269374\n",
      "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.301919\n",
      "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.412175\n",
      "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.347354\n",
      "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.166702\n",
      "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.226747\n",
      "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.062322\n",
      "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.336919\n",
      "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.254770\n",
      "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.139938\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0033, Accuracy: 9399/10000 (94%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.343860\n",
      "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.232214\n",
      "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.128403\n",
      "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.376491\n",
      "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.107786\n",
      "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.234693\n",
      "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.260924\n",
      "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.510795\n",
      "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.190818\n",
      "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.367318\n",
      "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.183091\n",
      "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.280501\n",
      "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.301376\n",
      "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.217376\n",
      "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.124394\n",
      "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.096530\n",
      "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.139977\n",
      "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.228406\n",
      "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.201199\n",
      "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.160346\n",
      "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.112008\n",
      "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.197248\n",
      "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.198772\n",
      "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.299482\n",
      "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.257270\n",
      "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.065506\n",
      "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.063358\n",
      "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.285075\n",
      "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.095217\n",
      "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.210655\n",
      "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.109763\n",
      "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.084414\n",
      "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.202360\n",
      "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.257777\n",
      "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.168830\n",
      "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.141460\n",
      "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.153821\n",
      "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.134729\n",
      "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.077951\n",
      "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.327470\n",
      "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.147481\n",
      "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.112028\n",
      "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.298833\n",
      "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.401105\n",
      "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.234579\n",
      "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.177263\n",
      "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.081246\n",
      "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.289799\n",
      "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.196051\n",
      "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.156223\n",
      "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.293667\n",
      "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.134687\n",
      "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.204964\n",
      "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.232661\n",
      "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.150694\n",
      "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.075914\n",
      "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.288972\n",
      "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.323043\n",
      "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.106034\n",
      "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.143615\n",
      "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.095684\n",
      "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.231416\n",
      "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.099018\n",
      "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.191351\n",
      "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.133529\n",
      "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.175482\n",
      "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.362525\n",
      "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.243594\n",
      "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.182209\n",
      "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.306439\n",
      "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.222201\n",
      "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.235027\n",
      "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.192729\n",
      "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.138970\n",
      "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.265494\n",
      "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.119741\n",
      "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.150293\n",
      "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.158552\n",
      "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.097806\n",
      "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.322064\n",
      "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.253332\n",
      "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.331888\n",
      "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.256331\n",
      "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.081579\n",
      "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.288533\n",
      "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.222290\n",
      "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.098882\n",
      "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.271131\n",
      "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.275223\n",
      "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.089801\n",
      "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.156321\n",
      "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.195767\n",
      "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.053891\n",
      "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.187413\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0026, Accuracy: 9515/10000 (95%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.191568\n",
      "Train Epoch: 6 | Batch Status: 640/60000 (1%) | Loss: 0.417926\n",
      "Train Epoch: 6 | Batch Status: 1280/60000 (2%) | Loss: 0.091459\n",
      "Train Epoch: 6 | Batch Status: 1920/60000 (3%) | Loss: 0.413580\n",
      "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.149393\n",
      "Train Epoch: 6 | Batch Status: 3200/60000 (5%) | Loss: 0.125508\n",
      "Train Epoch: 6 | Batch Status: 3840/60000 (6%) | Loss: 0.080134\n",
      "Train Epoch: 6 | Batch Status: 4480/60000 (7%) | Loss: 0.244064\n",
      "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.067792\n",
      "Train Epoch: 6 | Batch Status: 5760/60000 (10%) | Loss: 0.180085\n",
      "Train Epoch: 6 | Batch Status: 6400/60000 (11%) | Loss: 0.253321\n",
      "Train Epoch: 6 | Batch Status: 7040/60000 (12%) | Loss: 0.313359\n",
      "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.060286\n",
      "Train Epoch: 6 | Batch Status: 8320/60000 (14%) | Loss: 0.082897\n",
      "Train Epoch: 6 | Batch Status: 8960/60000 (15%) | Loss: 0.227899\n",
      "Train Epoch: 6 | Batch Status: 9600/60000 (16%) | Loss: 0.171392\n",
      "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.306590\n",
      "Train Epoch: 6 | Batch Status: 10880/60000 (18%) | Loss: 0.275616\n",
      "Train Epoch: 6 | Batch Status: 11520/60000 (19%) | Loss: 0.055131\n",
      "Train Epoch: 6 | Batch Status: 12160/60000 (20%) | Loss: 0.182003\n",
      "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.229439\n",
      "Train Epoch: 6 | Batch Status: 13440/60000 (22%) | Loss: 0.155386\n",
      "Train Epoch: 6 | Batch Status: 14080/60000 (23%) | Loss: 0.078193\n",
      "Train Epoch: 6 | Batch Status: 14720/60000 (25%) | Loss: 0.343330\n",
      "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.189204\n",
      "Train Epoch: 6 | Batch Status: 16000/60000 (27%) | Loss: 0.099151\n",
      "Train Epoch: 6 | Batch Status: 16640/60000 (28%) | Loss: 0.120877\n",
      "Train Epoch: 6 | Batch Status: 17280/60000 (29%) | Loss: 0.080663\n",
      "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.140325\n",
      "Train Epoch: 6 | Batch Status: 18560/60000 (31%) | Loss: 0.167315\n",
      "Train Epoch: 6 | Batch Status: 19200/60000 (32%) | Loss: 0.061505\n",
      "Train Epoch: 6 | Batch Status: 19840/60000 (33%) | Loss: 0.179167\n",
      "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.085305\n",
      "Train Epoch: 6 | Batch Status: 21120/60000 (35%) | Loss: 0.255196\n",
      "Train Epoch: 6 | Batch Status: 21760/60000 (36%) | Loss: 0.205568\n",
      "Train Epoch: 6 | Batch Status: 22400/60000 (37%) | Loss: 0.130512\n",
      "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.191626\n",
      "Train Epoch: 6 | Batch Status: 23680/60000 (39%) | Loss: 0.257946\n",
      "Train Epoch: 6 | Batch Status: 24320/60000 (41%) | Loss: 0.076807\n",
      "Train Epoch: 6 | Batch Status: 24960/60000 (42%) | Loss: 0.247158\n",
      "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.397514\n",
      "Train Epoch: 6 | Batch Status: 26240/60000 (44%) | Loss: 0.154921\n",
      "Train Epoch: 6 | Batch Status: 26880/60000 (45%) | Loss: 0.168504\n",
      "Train Epoch: 6 | Batch Status: 27520/60000 (46%) | Loss: 0.211231\n",
      "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.047582\n",
      "Train Epoch: 6 | Batch Status: 28800/60000 (48%) | Loss: 0.069241\n",
      "Train Epoch: 6 | Batch Status: 29440/60000 (49%) | Loss: 0.138861\n",
      "Train Epoch: 6 | Batch Status: 30080/60000 (50%) | Loss: 0.098097\n",
      "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.056516\n",
      "Train Epoch: 6 | Batch Status: 31360/60000 (52%) | Loss: 0.068918\n",
      "Train Epoch: 6 | Batch Status: 32000/60000 (53%) | Loss: 0.115106\n",
      "Train Epoch: 6 | Batch Status: 32640/60000 (54%) | Loss: 0.187298\n",
      "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.243021\n",
      "Train Epoch: 6 | Batch Status: 33920/60000 (57%) | Loss: 0.078109\n",
      "Train Epoch: 6 | Batch Status: 34560/60000 (58%) | Loss: 0.022993\n",
      "Train Epoch: 6 | Batch Status: 35200/60000 (59%) | Loss: 0.311821\n",
      "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.133754\n",
      "Train Epoch: 6 | Batch Status: 36480/60000 (61%) | Loss: 0.159744\n",
      "Train Epoch: 6 | Batch Status: 37120/60000 (62%) | Loss: 0.104943\n",
      "Train Epoch: 6 | Batch Status: 37760/60000 (63%) | Loss: 0.177977\n",
      "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.124713\n",
      "Train Epoch: 6 | Batch Status: 39040/60000 (65%) | Loss: 0.287733\n",
      "Train Epoch: 6 | Batch Status: 39680/60000 (66%) | Loss: 0.110292\n",
      "Train Epoch: 6 | Batch Status: 40320/60000 (67%) | Loss: 0.057438\n",
      "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.054080\n",
      "Train Epoch: 6 | Batch Status: 41600/60000 (69%) | Loss: 0.232215\n",
      "Train Epoch: 6 | Batch Status: 42240/60000 (70%) | Loss: 0.106315\n",
      "Train Epoch: 6 | Batch Status: 42880/60000 (71%) | Loss: 0.154825\n",
      "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.056155\n",
      "Train Epoch: 6 | Batch Status: 44160/60000 (74%) | Loss: 0.142712\n",
      "Train Epoch: 6 | Batch Status: 44800/60000 (75%) | Loss: 0.121620\n",
      "Train Epoch: 6 | Batch Status: 45440/60000 (76%) | Loss: 0.088875\n",
      "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.052906\n",
      "Train Epoch: 6 | Batch Status: 46720/60000 (78%) | Loss: 0.249703\n",
      "Train Epoch: 6 | Batch Status: 47360/60000 (79%) | Loss: 0.145413\n",
      "Train Epoch: 6 | Batch Status: 48000/60000 (80%) | Loss: 0.126461\n",
      "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.143346\n",
      "Train Epoch: 6 | Batch Status: 49280/60000 (82%) | Loss: 0.077434\n",
      "Train Epoch: 6 | Batch Status: 49920/60000 (83%) | Loss: 0.140558\n",
      "Train Epoch: 6 | Batch Status: 50560/60000 (84%) | Loss: 0.047231\n",
      "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.060661\n",
      "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.132412\n",
      "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.135215\n",
      "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.134012\n",
      "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.052980\n",
      "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.187822\n",
      "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.154841\n",
      "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.062879\n",
      "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.106835\n",
      "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.097120\n",
      "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.167530\n",
      "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.155810\n",
      "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.098813\n",
      "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.168840\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0022, Accuracy: 9596/10000 (96%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.150937\n",
      "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.048927\n",
      "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.111935\n",
      "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.082296\n",
      "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.039818\n",
      "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.053432\n",
      "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.243229\n",
      "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.088226\n",
      "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.198194\n",
      "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.092094\n",
      "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.055732\n",
      "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.040022\n",
      "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.151757\n",
      "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.080195\n",
      "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.075152\n",
      "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.136835\n",
      "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.096042\n",
      "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.144623\n",
      "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.199045\n",
      "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.058447\n",
      "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.169545\n",
      "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.081242\n",
      "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.111212\n",
      "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.156916\n",
      "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.210317\n",
      "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.078901\n",
      "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.077272\n",
      "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.060737\n",
      "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.051457\n",
      "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.112566\n",
      "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.096968\n",
      "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.232291\n",
      "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.065794\n",
      "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.047842\n",
      "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.050510\n",
      "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.042017\n",
      "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.111802\n",
      "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.108634\n",
      "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.128111\n",
      "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.162560\n",
      "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.114767\n",
      "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.121445\n",
      "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.113432\n",
      "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.108519\n",
      "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.114983\n",
      "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.205803\n",
      "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.117360\n",
      "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.072990\n",
      "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.238913\n",
      "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.058609\n",
      "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.148087\n",
      "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.061218\n",
      "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.086135\n",
      "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.158866\n",
      "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.137956\n",
      "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.088363\n",
      "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.091096\n",
      "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.166003\n",
      "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.042043\n",
      "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.108214\n",
      "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.043009\n",
      "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.114832\n",
      "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.253737\n",
      "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.079113\n",
      "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.073613\n",
      "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.104565\n",
      "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.087527\n",
      "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.064588\n",
      "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.231043\n",
      "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.073025\n",
      "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.076932\n",
      "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.081850\n",
      "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.236629\n",
      "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.273394\n",
      "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.167857\n",
      "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.090878\n",
      "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.071047\n",
      "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.191568\n",
      "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.173344\n",
      "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.027131\n",
      "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.290789\n",
      "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.025270\n",
      "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.115813\n",
      "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.058257\n",
      "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.036394\n",
      "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.055374\n",
      "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.155541\n",
      "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.202661\n",
      "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.137579\n",
      "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.124985\n",
      "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.116179\n",
      "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.234639\n",
      "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.269697\n",
      "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.229636\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0021, Accuracy: 9630/10000 (96%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.066969\n",
      "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.185124\n",
      "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.103896\n",
      "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.028867\n",
      "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.096572\n",
      "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.111768\n",
      "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.160571\n",
      "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.052005\n",
      "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.056848\n",
      "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.057431\n",
      "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.148747\n",
      "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.053261\n",
      "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.165523\n",
      "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.108151\n",
      "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.077468\n",
      "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.040042\n",
      "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.083443\n",
      "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.068398\n",
      "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.126987\n",
      "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.095792\n",
      "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.084563\n",
      "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.030297\n",
      "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.180060\n",
      "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.095345\n",
      "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.037535\n",
      "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.057738\n",
      "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.148722\n",
      "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.028002\n",
      "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.156284\n",
      "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.125262\n",
      "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.080203\n",
      "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.141439\n",
      "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.076243\n",
      "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.072600\n",
      "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.097066\n",
      "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.091154\n",
      "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.090154\n",
      "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.096389\n",
      "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.187938\n",
      "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.051059\n",
      "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.113039\n",
      "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.040859\n",
      "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.116595\n",
      "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.198356\n",
      "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.159032\n",
      "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.039572\n",
      "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.124271\n",
      "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.220839\n",
      "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.132998\n",
      "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.055588\n",
      "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.100051\n",
      "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.046464\n",
      "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.124993\n",
      "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.112555\n",
      "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.035818\n",
      "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.045461\n",
      "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.101369\n",
      "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.126223\n",
      "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.240452\n",
      "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.123056\n",
      "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.124826\n",
      "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.145045\n",
      "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.039913\n",
      "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.064244\n",
      "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.075215\n",
      "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.115953\n",
      "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.022231\n",
      "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.108175\n",
      "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.082543\n",
      "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.111392\n",
      "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.060493\n",
      "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.168326\n",
      "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.096901\n",
      "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.099920\n",
      "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.035213\n",
      "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.077989\n",
      "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.040937\n",
      "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.181915\n",
      "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.037807\n",
      "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.105005\n",
      "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.043316\n",
      "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.064169\n",
      "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.240577\n",
      "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.123709\n",
      "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.300606\n",
      "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.072694\n",
      "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.032992\n",
      "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.035763\n",
      "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.101003\n",
      "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.030220\n",
      "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.112033\n",
      "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.038771\n",
      "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.197921\n",
      "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.196919\n",
      "Training time: 0m 11s\n",
      "===========================\n",
      "Test set: Average loss: 0.0021, Accuracy: 9610/10000 (96%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.015684\n",
      "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.028481\n",
      "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.209599\n",
      "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.104735\n",
      "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.076616\n",
      "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.118127\n",
      "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.071195\n",
      "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.028717\n",
      "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.029991\n",
      "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.118867\n",
      "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.041554\n",
      "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.082646\n",
      "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.092387\n",
      "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.056235\n",
      "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.069096\n",
      "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.141215\n",
      "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.096961\n",
      "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.256913\n",
      "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.110336\n",
      "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.098136\n",
      "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.032971\n",
      "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.037725\n",
      "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.189624\n",
      "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.175773\n",
      "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.054484\n",
      "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.092401\n",
      "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.083576\n",
      "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.045100\n",
      "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.066739\n",
      "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.085074\n",
      "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.102842\n",
      "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.032388\n",
      "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.038595\n",
      "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.043703\n",
      "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.190300\n",
      "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.074187\n",
      "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.074044\n",
      "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.027554\n",
      "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.076976\n",
      "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.093544\n",
      "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.019621\n",
      "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.034650\n",
      "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.042224\n",
      "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.150409\n",
      "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.199686\n",
      "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.120347\n",
      "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.177661\n",
      "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.019896\n",
      "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.047169\n",
      "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.144402\n",
      "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.193729\n",
      "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.205075\n",
      "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.039371\n",
      "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.038442\n",
      "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.054378\n",
      "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.088391\n",
      "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.037235\n",
      "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.128489\n",
      "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.032341\n",
      "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.068368\n",
      "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.048471\n",
      "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.089069\n",
      "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.140273\n",
      "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.104233\n",
      "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.023643\n",
      "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.073439\n",
      "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.118836\n",
      "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.127727\n",
      "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.015956\n",
      "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.087031\n",
      "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.053511\n",
      "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.172960\n",
      "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.039605\n",
      "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.206351\n",
      "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.055563\n",
      "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.037530\n",
      "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.042322\n",
      "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.068556\n",
      "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.361532\n",
      "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.056015\n",
      "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.109785\n",
      "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.162269\n",
      "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.195276\n",
      "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.043140\n",
      "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.222259\n",
      "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.140054\n",
      "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.043041\n",
      "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.035200\n",
      "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.110762\n",
      "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.101608\n",
      "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.086552\n",
      "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.021303\n",
      "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.202007\n",
      "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.113662\n",
      "Training time: 0m 11s\n",
      "===========================\n",
      "Test set: Average loss: 0.0016, Accuracy: 9699/10000 (97%)\n",
      "Testing time: 0m 12s\n",
      "Total Time: 1m 45s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
